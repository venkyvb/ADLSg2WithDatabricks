{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"90e0c5e4-89fa-40e1-8875-9c7739f8d1df","showTitle":false,"title":""}},"source":["### Requirements/Initial Setup\n","\n","1. Create an Application in AAD - https://portal.azure.com/#view/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/~/Overview\n","2. Note the `ClientId`, `Tenant/DirectoryId`, `SecretValue`, `SecretId`\n","3. Go to the Storage Account -> IAM role and assign the role  `Storage Blob Data Contributor` to the application (via the option `Assign access to User, group or service principal`)\n","4. Create a new Azure Key Vault (for detailed screenshots refer to https://hevodata.com/learn/databricks-secret/)\n","5. Create a Secret and enter the `SecretValue` from (2) above and note down the Secret Name (e.g. `BlobStoreAccessKey`)\n","6. Go to the Databricks URL e.g. https://adb-<some_id>.azuredatabricks.net/?o=<some_id> and add `#secrets/createScope` to the URL to create a Databricks scope. (e.g. `azure_delta_lake_access_scope`)\n","\n","Add the following spark configs (pyspark) to allow the access to the storage account. (for additional options refer to - https://docs.databricks.com/storage/azure-storage.html)\n","\n","```\n","service_credential = dbutils.secrets.get(scope=\"<scope>\",key=\"<service-credential-key>\")\n","\n","spark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\n","spark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<application-id>\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", service_credential)\n","spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6ad4fd05-4a3e-4862-b3b2-2527e803da57","showTitle":false,"title":""}},"outputs":[],"source":["\n","dbutils.fs.ls(\"abfss://CONTAINER@STORAGE_ACCOUNT.dfs.core.windows.net/<path>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"a76d4630-d62c-4929-9cee-cdd1a1e8351d","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","SELECT patient_city, gender, COUNT(count) AS `Number of Patients`\n","    FROM delta.`abfss://CONTAINER@STORAGE_ACCOUNT.dfs.core.windows.net/delta_lake_poc/gold`\n","    GROUP BY patient_city, gender"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fe939253-79ba-40db-ad4e-e6c6e125c0b4","showTitle":false,"title":""}},"source":["### Register External tables with Databricks\n","To register the able in Databricks catalog (can be either Unity or Hive metastore) the table should be created as an EXTERNAL table (aka Unmanaged table). With this registration, only the metadata is registered with Databricks and the data is still in the ADLSg2 folders. With this even if the table is dropped in the Databricks catalog the data is not deleted. See - https://docs.databricks.com/lakehouse/data-objects.html#what-is-an-unmanaged-table for further details.\n","\n","1. Create metastore - To register the unmanaged table a Metastore and Catalog needs to be created first. For example you can go to the `Manage Account` option and click on the `Data` icon/option to create a new Unity metastore. (say this mwetastore name is - `main`). In order to create a metastore, you would need to go to Azure portal and first create an `Access Connector for Azure Databricks`. When creating a metastore in Unity you would need to enter the ADLSg2 folder path as well as the Access Connector Id (resourceId for the Access Connector that was just created in Azure). Note that currently Unity catalog only allows 1 metastore per region, so please define the ADLSg2 path accordingly.\n","2. Assign one or more Databricks workspaces that neeed to use the metastore to the created metastore.\n","3. The Databricks cluster also would need to be assigned to work with Unity catalog. This can be done by just creating a new cluster.\n","4. Register an External location to point to the ADLSg2 folder that needs to be accessed via the metastore.  \n","5. Once then follow the steps below to create the table.\n","\n","#### Create an External Location in Unity Catalog\n","Before you can create an external table, you must create a storage credential that allows Unity Catalog to read from and write to the path on your cloud tenant, and an external location that references it.\n","##### Create a storage credential\n","Follow these steps to create a storage credential using the Data Explorer.\n","\n","1. In the sidebar, set the persona to SQL.\n","2. Click Data.\n","3. Click Storage Credentials.\n","4. Click Create Credential.\n","5. Enter `azureblobstorekey` for he name of the storage credential.\n","6. Set `SecretValue`, `Tenant/DirectoryId`, and `ClientId` to the values for your service principal.\n","7 Optionally enter a comment for the storage credential.\n","8. Click Save.\n","\n","##### Create an external location\n","An external location references a storage credential (created above) and points to an ADLSg2 folder. The external location allows reading from and writing to only that path and its child directories. Follow these steps to create an external location using the Data Explorer.\n","\n","1. Click Data.\n","2. Click External Locations.\n","3. Click Create location.\n","4. Enter `deltatablepoc` for the name of the external location.\n","5. Enter the storage container path for the location allows reading from or writing to.\n","6. Set Storage Credential to example_credential to the storage credential you just created.\n","7. Optionally enter a comment for the external location.\n","8. Click Save."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"d8b596fc-fe35-4da4-b96f-535c48aff2f5","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","\n","-- Create external table with Unity catalog metastore\n","CREATE TABLE IF NOT EXISTS main.default.patientGold USING DELTA LOCATION 'abfss://CONTAINER@STORAGE_ACCOUNT.dfs.core.windows.net/delta_lake_poc/gold';\n","\n","SELECT patient_city, gender, COUNT(count) AS `Number of Patients`\n","    FROM main.default.patientGold\n","    GROUP BY patient_city, gender"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"1efb1a9f-1f18-4ae3-ab4c-6d11866c4daa","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","\n","CREATE TABLE IF NOT EXISTS main.default.patientSilver USING DELTA LOCATION 'abfss://CONTAINER@STORAGE_ACCOUNT.dfs.core.windows.net/delta_lake_poc/silver';\n","\n","SELECT * FROM main.default.patientSilver;"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"implicitDf":true},"inputWidgets":{},"nuid":"1588a6fc-ed45-4f13-b28b-e9753eba3303","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","\n","-- Create external table on Hive metastore\n","-- NOTE with Hive metastore dropping that schema using the CASCADE option causes all files in that schema location to be deleted recursively, regardless of the table type (managed or external).\n","\n","CREATE TABLE IF NOT EXISTS patientGold4 USING DELTA LOCATION 'abfss://CONTAINER@STORAGE_ACCOUNT.dfs.core.windows.net/delta_lake_poc/gold';\n","\n","SELECT patient_city, gender, COUNT(count) AS `Number of Patients`\n","    FROM patientGold4\n","    GROUP BY patient_city, gender"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1129531034360025,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"AzureStorageAccessTest","notebookOrigID":1734121157704775,"widgets":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13"},"vscode":{"interpreter":{"hash":"46ca1803c51943936bd651f2c4c8de4f1ac127aed242d81dda93e87b506b9c83"}}},"nbformat":4,"nbformat_minor":0}
